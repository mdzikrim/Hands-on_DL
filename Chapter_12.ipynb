{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvD/TfQX5jt6zwk5p0+5yo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdzikrim/Hands-on_DL/blob/main/Chapter_12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Vz6cqmeQ_dfG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class CustomLayerNorm(layers.Layer):\n",
        "    def build(self, input_shape):\n",
        "        self.alpha = self.add_weight(name=\"alpha\", shape=[input_shape[-1]], initializer=\"ones\", trainable=True)\n",
        "        self.beta = self.add_weight(name=\"beta\", shape=[input_shape[-1]], initializer=\"zeros\", trainable=True)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        mean, var = tf.nn.moments(inputs, axes=-1, keepdims=True)\n",
        "        epsilon = 1e-5\n",
        "        norm = (inputs - mean) / tf.sqrt(var + epsilon)\n",
        "        return self.alpha * norm + self.beta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LayerNormalization\n",
        "import numpy as np\n",
        "\n",
        "# Test layer\n",
        "sample = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "\n",
        "custom_ln = CustomLayerNorm()\n",
        "keras_ln = LayerNormalization()\n",
        "\n",
        "out1 = custom_ln(sample)\n",
        "out2 = keras_ln(sample)\n",
        "\n",
        "print(\"Custom LayerNorm Output:\\n\", out1.numpy())\n",
        "print(\"Keras LayerNorm Output:\\n\", out2.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDvTAvOL_jMz",
        "outputId": "f7d17ef8-2b1c-4459-b889-5a6e15a002df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom LayerNorm Output:\n",
            " [[-1.2247356  0.         1.2247356]\n",
            " [-1.2247356  0.         1.2247356]]\n",
            "Keras LayerNorm Output:\n",
            " [[-1.2238274  0.         1.2238274]\n",
            " [-1.2238274  0.         1.2238274]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_train, X_valid = X_train_full[:50000], X_train_full[50000:]\n",
        "y_train, y_valid = y_train_full[:50000], y_train_full[50000:]\n",
        "\n",
        "X_train = X_train[..., tf.newaxis]\n",
        "X_valid = X_valid[..., tf.newaxis]\n",
        "X_test = X_test[..., tf.newaxis]\n"
      ],
      "metadata": {
        "id": "LY14dRjh_kjY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = layers.Flatten()\n",
        "        self.d1 = layers.Dense(256, activation=\"relu\")\n",
        "        self.d2 = layers.Dense(10)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.d1(x)\n",
        "        return self.d2(x)\n"
      ],
      "metadata": {
        "id": "xkeJJjBe_meF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(32)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean()\n",
        "train_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "val_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "EPOCHS = 5\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    for X_batch, y_batch in train_ds:\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(X_batch)\n",
        "            loss = loss_fn(y_batch, logits)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_acc(y_batch, logits)\n",
        "\n",
        "    for X_batch, y_batch in val_ds:\n",
        "        val_logits = model(X_batch)\n",
        "        val_acc.update_state(y_batch, val_logits)\n",
        "\n",
        "    print(f\"Train loss: {train_loss.result():.4f} | Train acc: {train_acc.result():.4f} | Val acc: {val_acc.result():.4f}\")\n",
        "    train_loss.reset_state()\n",
        "    train_acc.reset_state()\n",
        "    val_acc.reset_state()\n"
      ],
      "metadata": {
        "id": "Rx-h1WJt_nyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfb2a23d-d4da-4b48-c32c-addc889c5347"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Train loss: 0.4980 | Train acc: 0.8236 | Val acc: 0.8282\n",
            "\n",
            "Epoch 2\n",
            "Train loss: 0.3751 | Train acc: 0.8640 | Val acc: 0.8681\n",
            "\n",
            "Epoch 3\n",
            "Train loss: 0.3352 | Train acc: 0.8778 | Val acc: 0.8744\n",
            "\n",
            "Epoch 4\n",
            "Train loss: 0.3081 | Train acc: 0.8869 | Val acc: 0.8777\n",
            "\n",
            "Epoch 5\n",
            "Train loss: 0.2869 | Train acc: 0.8926 | Val acc: 0.8812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler: low LR for early layers, high for last\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9)\n",
        "model = MyModel()\n",
        "\n",
        "@tf.function\n",
        "def train_step(X_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(X_batch)\n",
        "        loss = loss_fn(y_batch, logits)\n",
        "    grads = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return loss\n",
        "\n",
        "# Reuse train_ds from earlier\n",
        "for epoch in range(3):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    for step, (X_batch, y_batch) in enumerate(train_ds):\n",
        "        loss = train_step(X_batch, y_batch)\n",
        "        if step % 200 == 0:\n",
        "            print(f\"Step {step}: Loss = {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZrfHnCYJ_pH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb495cb-ef2c-47e0-eacf-30256b95219e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "Step 0: Loss = 2.4138\n",
            "Step 200: Loss = 0.6160\n",
            "Step 400: Loss = 0.6867\n",
            "Step 600: Loss = 0.6339\n",
            "Step 800: Loss = 0.4408\n",
            "Step 1000: Loss = 0.5159\n",
            "Step 1200: Loss = 0.5663\n",
            "Step 1400: Loss = 0.2888\n",
            "\n",
            "Epoch 2\n",
            "Step 0: Loss = 0.4457\n",
            "Step 200: Loss = 0.6214\n",
            "Step 400: Loss = 0.5594\n",
            "Step 600: Loss = 0.3266\n",
            "Step 800: Loss = 0.4707\n",
            "Step 1000: Loss = 0.3948\n",
            "Step 1200: Loss = 0.2817\n",
            "Step 1400: Loss = 0.4058\n",
            "\n",
            "Epoch 3\n",
            "Step 0: Loss = 0.5149\n",
            "Step 200: Loss = 0.3599\n",
            "Step 400: Loss = 0.4600\n",
            "Step 600: Loss = 0.3892\n",
            "Step 800: Loss = 0.2241\n",
            "Step 1000: Loss = 0.4088\n",
            "Step 1200: Loss = 0.2698\n",
            "Step 1400: Loss = 0.2415\n"
          ]
        }
      ]
    }
  ]
}